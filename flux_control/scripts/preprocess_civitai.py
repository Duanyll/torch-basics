import os
import argparse
import lmdb
import pickle
import numpy as np
import diffusers
import transformers
from diffusers.pipelines.flux.pipeline_flux import FluxPipeline
import torch
import torch.multiprocessing as mp
from PIL import Image
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
from rich import print

DTYPE = torch.bfloat16


def process_sample(image_path, prompt_path, pipe, device):
    """å¤„ç†å•ä¸ªæ ·æœ¬å¹¶è¿”å›å¤„ç†åçš„æ•°æ®"""
    try:
        with torch.no_grad():
            img = Image.open(image_path)
            img = pipe.image_processor.preprocess(img).to(device=device, dtype=DTYPE)
            latent = pipe.vae.encode(img).latent_dist.sample()
            latent = (
                latent - pipe.vae.config.shift_factor
            ) * pipe.vae.config.scaling_factor

            with open(prompt_path, "r") as f:
                prompt = f.read().strip()

            prompt_embeds, pooled_prompt_embeds, _ = pipe.encode_prompt(
                prompt=prompt,
                prompt_2=prompt,
                device=device,
            )

        return {
            "clean_latents": latent.cpu().squeeze(0),
            "prompt_embeds": prompt_embeds.cpu().squeeze(0),
            "pooled_prompt_embeds": pooled_prompt_embeds.cpu().squeeze(0),
        }
    except Exception as e:
        raise RuntimeError(f"Error processing {image_path}") from e


def worker_process(file_chunk, gpu_id, result_queue):
    """å·¥ä½œè¿›ç¨‹ï¼Œå¤„ç†åˆ†é…çš„æ–‡ä»¶å—"""
    try:
        torch.cuda.set_device(gpu_id)
        device = torch.device(f"cuda:{gpu_id}")

        diffusers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.disable_progress_bar()
        transformers.utils.logging.set_verbosity_error()
        transformers.utils.logging.disable_progress_bar()
        
        # åˆå§‹åŒ–æ¨¡å‹
        pipe = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev", torch_dtype=DTYPE, transformer=None
        ).to(device)

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶å¯¹
        for image_path, prompt_path in file_chunk:
            try:
                sample = process_sample(image_path, prompt_path, pipe, device)
                result_queue.put(("success", sample, image_path))
            except Exception as e:
                result_queue.put(("error", str(e), image_path))

        # å‘é€å®Œæˆä¿¡å·
        result_queue.put(("done", gpu_id, None))

    except Exception as e:
        result_queue.put(("fatal", str(e), None))


def collect_file_pairs(directory):
    """æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„å›¾åƒ-æ–‡æœ¬å¯¹"""
    file_pairs = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith((".jpeg", ".jpg", ".png")):
                image_path = os.path.join(root, file)
                base_name = os.path.splitext(image_path)[0]
                prompt_path = base_name + ".txt"
                if os.path.exists(prompt_path):
                    file_pairs.append((image_path, prompt_path))
    return file_pairs


def main():
    parser = argparse.ArgumentParser(
        description="å¤šGPUåŠ é€Ÿå¤„ç†å›¾åƒæ•°æ®åˆ°LMDB",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("input_dir", help="åŒ…å«å›¾åƒå’Œæ–‡æœ¬æ–‡ä»¶çš„è¾“å…¥ç›®å½•")
    parser.add_argument("output_lmdb", help="è¾“å‡ºLMDBæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--gpus", default="0", help="ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”")
    args = parser.parse_args()

    # åˆå§‹åŒ–è®¾ç½®
    gpu_ids = list(map(int, args.gpus.split(",")))
    file_pairs = collect_file_pairs(args.input_dir)
    chunks = np.array_split(file_pairs, len(gpu_ids))

    print(f"[bold green]ğŸš€ å¯åŠ¨å¤„ç†:[/bold green]")
    print(f"â€¢ æ‰¾åˆ° [bold]{len(file_pairs)}[/bold] ä¸ªæœ‰æ•ˆæ–‡ä»¶å¯¹")
    print(f"â€¢ ä½¿ç”¨GPU: [bold]{args.gpus}[/bold]")
    print(f"â€¢ è¾“å‡ºè·¯å¾„: [bold]{args.output_lmdb}[/bold]")

    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    ctx = mp.get_context("spawn")
    manager = ctx.Manager()
    result_queue = manager.Queue()

    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
    processes = []
    for i, (chunk, gpu_id) in enumerate(zip(chunks, gpu_ids)):
        p = ctx.Process(target=worker_process, args=(chunk, gpu_id, result_queue))
        p.start()
        processes.append(p)

    # åˆå§‹åŒ–è¿›åº¦æ¡
    progress = Progress(
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        TimeRemainingColumn()
    )

    success_count = 0
    error_count = 0
    completed_gpus = set()

    with progress:
        task = progress.add_task("å¤„ç†æ–‡ä»¶ä¸­", total=len(file_pairs))

        # åˆ›å»ºLMDBç¯å¢ƒ
        env = lmdb.open(args.output_lmdb, map_size=int(1e12), readonly=False)

        while True:
            result = result_queue.get()
            status = result[0]

            if status == "success":
                _, sample, path = result
                with env.begin(write=True) as txn:
                    key = f"{success_count:08}".encode()
                    txn.put(key, pickle.dumps(sample))
                success_count += 1
                progress.update(
                    task,
                    advance=1,
                    description=f"å¤„ç†æ–‡ä»¶: {success_count}/{len(file_pairs)}",
                )

            elif status == "error":
                _, error_msg, path = result
                print(f"[bold red]âš ï¸ é”™è¯¯å¤„ç†æ–‡ä»¶ {path}:[/bold red]\n{error_msg}")
                error_count += 1
                progress.update(task, advance=1)

            elif status == "done":
                _, gpu_id, _ = result
                completed_gpus.add(gpu_id)
                if len(completed_gpus) == len(gpu_ids):
                    break

            elif status == "fatal":
                _, error_msg, _ = result
                print(f"[bold red]ğŸ’¥ è‡´å‘½é”™è¯¯:[/bold red] {error_msg}")
                break

    # æ¸…ç†è¿›ç¨‹
    for p in processes:
        p.join()
        p.close()

    print(f"\n[bold green]âœ… å¤„ç†å®Œæˆ![/bold green]")
    print(f"â€¢ æˆåŠŸå¤„ç†: [bold green]{success_count}[/bold green]")
    print(f"â€¢ å¤±è´¥æ–‡ä»¶: [bold red]{error_count}[/bold red]")
    print(f"â€¢ è¾“å‡ºè·¯å¾„: [bold]{args.output_lmdb}[/bold]")


if __name__ == "__main__":
    main()
