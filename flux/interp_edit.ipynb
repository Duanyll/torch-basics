{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import (\n",
    "    FluxTransformer2DModel,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxPipeline,\n",
    "    AutoencoderKL,\n",
    "    AutoencoderTiny,\n",
    ")\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat, reduce\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load the memory optimized Flux.1-Dev model. The model weights are downcasted to fp8 but activations are still in bf16. With the cached layerwise upcasting and gradient checkpointing trick, it is possible to back-propagate through the model on 24GB GPUs.\n",
    "\n",
    "Memory footprint:\n",
    "\n",
    "- T5 encoder weights: 9.5GB (will be automatically offloaded when not needed)\n",
    "- Flux transformer weights: 12GB\n",
    "- Activations for backward (to `hidden_states`): 1.65GB (gradient checkpointing enabled)\n",
    "\n",
    "TAEF1 is used for live preview of the sampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import create_low_vram_flux_pipeline, freeze_pipeline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.bfloat16\n",
    "pipe = create_low_vram_flux_pipeline(device)\n",
    "freeze_pipeline(pipe)\n",
    "taef1 = AutoencoderTiny.from_pretrained(\"madebyollin/taef1\", torch_dtype=dtype).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and Latent Space\n",
    "\n",
    "In the implementation of Flux from diffusers, the image tensor is in BCHW format, raneged in $[-1, 1]$. The VAE latent space has 16 channels, and spatial resolution is reduced 8 times from the input image. However, the transformer model require 2x2 pixelshuffled input as its `hidden_states`. Then the `hidden_states` are in BND format, where N is $H\\times W/256$ and D is 64.\n",
    "\n",
    "In the following cells, `latent` refers to the transformer's `hidden_states`, and `image` refers to decoded image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 1024\n",
    "width = 1024\n",
    "\n",
    "\n",
    "def show_image_tensor(x):\n",
    "    x = x.detach().cpu()\n",
    "    return pipe.image_processor.postprocess(x, output_type=\"pil\")[0]\n",
    "\n",
    "\n",
    "def load_image_tensor(path):\n",
    "    img = Image.open(path)\n",
    "    img = pipe.image_processor.preprocess(img, height=height, width=width)\n",
    "    return img.to(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def image_to_latent(img: torch.Tensor):\n",
    "    vae: AutoencoderKL = pipe.vae\n",
    "    latent: torch.Tensor = vae.encode(img).latent_dist.sample()\n",
    "    latent = (latent - vae.config.shift_factor) * vae.config.scaling_factor\n",
    "    latent = rearrange(latent, \"b c (h nh) (w nw) -> b (h w) (c nh nw)\", nh=2, nw=2)\n",
    "    return latent\n",
    "\n",
    "\n",
    "def make_empty_latent(height: int, width: int):\n",
    "    latent = torch.randn(\n",
    "        1,\n",
    "        (height * width // 256),\n",
    "        pipe.transformer.config.in_channels,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    return latent\n",
    "\n",
    "\n",
    "def latent_to_image(\n",
    "    latent: torch.Tensor, height: int, width: int, vae: AutoencoderKL = None\n",
    "):\n",
    "    if vae is None:\n",
    "        vae = pipe.vae\n",
    "    n = pipe.vae_scale_factor * 2\n",
    "    latent = rearrange(\n",
    "        latent,\n",
    "        \"b (h w) (c nh nw) -> b c (h nh) (w nw)\",\n",
    "        h=height // n,\n",
    "        w=width // n,\n",
    "        nh=2,\n",
    "        nw=2,\n",
    "    )\n",
    "    latent = latent / vae.config.scaling_factor + vae.config.shift_factor\n",
    "    img: torch.Tensor = vae.decode(latent, return_dict=False)[0]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image_tensor(\"assets/editing_cat.png\")\n",
    "latent_img = image_to_latent(img)\n",
    "img_decoded = latent_to_image(latent_img, height, width)\n",
    "show_image_tensor(img_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gallery = []\n",
    "\n",
    "def add_gallery(img, tag):\n",
    "    global _gallery\n",
    "    _gallery.append((show_image_tensor(img), tag))\n",
    "\n",
    "def show_gallery(n_cols=6):\n",
    "    global _gallery\n",
    "    n = len(_gallery)\n",
    "    if n == 0:\n",
    "        print(\"Nothing to show\")\n",
    "        return\n",
    "    if n < n_cols:\n",
    "        n_cols = n\n",
    "    n_rows = math.ceil(n / n_cols)\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 3, n_rows * 3.2))\n",
    "    for i, (img, tag) in enumerate(_gallery):\n",
    "        ax = axs[i]\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(tag)\n",
    "    fig.subplots_adjust(hspace=0.01, wspace=0.01)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    _gallery = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_timestep_shift(t: torch.Tensor, latent_len: int):\n",
    "    scfg = pipe.scheduler.config\n",
    "    m = (scfg.max_shift - scfg.base_shift) / (\n",
    "        scfg.max_image_seq_len - scfg.base_image_seq_len\n",
    "    )\n",
    "    b = scfg.base_shift - m * scfg.base_image_seq_len\n",
    "    mu = m * latent_len + b\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 28\n",
    "t = torch.linspace(1.0, 0.0, n_timesteps + 1)\n",
    "t = calculate_timestep_shift(t, latent_img.shape[1])\n",
    "plt.plot(t.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_velocity_function(prompt: str, height: int, width: int):\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt,\n",
    "        device=device,\n",
    "    )\n",
    "    n = pipe.vae_scale_factor * 2\n",
    "    latent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "        batch_size=1, height=height // n, width=width // n, device=device, dtype=dtype\n",
    "    )\n",
    "    guidance = torch.tensor([3.5], device=device, dtype=dtype)\n",
    "\n",
    "    def velocity(latent: torch.Tensor, t: torch.Tensor):\n",
    "        transformer: FluxTransformer2DModel = pipe.transformer\n",
    "        if isinstance(t, float):\n",
    "            timestep = torch.tensor([t], device=device, dtype=dtype)\n",
    "        elif t.dim() == 0:\n",
    "            timestep = t.unsqueeze(0)\n",
    "        noise_pred = transformer(\n",
    "            hidden_states=latent,\n",
    "            timestep=timestep,\n",
    "            guidance=guidance,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "        )[0]\n",
    "        return noise_pred\n",
    "\n",
    "    return velocity\n",
    "\n",
    "\n",
    "def sample_preview(latent, velocity, t):\n",
    "    n_timesteps = len(t) - 1\n",
    "    latent_display = display(\n",
    "        show_image_tensor(latent_to_image(latent, height, width, vae=taef1)),\n",
    "        display_id=\"latent_preview\",\n",
    "    )\n",
    "    for i in tqdm(range(n_timesteps)):\n",
    "        noise_pred = velocity(latent, t[i])\n",
    "        latent_hp = latent.to(torch.float32)\n",
    "        noise_pred_hp = noise_pred.to(torch.float32)\n",
    "        latent_hp = latent_hp + noise_pred_hp * (t[i + 1] - t[i])\n",
    "        latent = latent_hp.to(dtype)\n",
    "        latent_display.update(\n",
    "            show_image_tensor(latent_to_image(latent, height, width, vae=taef1))\n",
    "        )\n",
    "    latent_display.update(show_image_tensor(latent_to_image(latent, height, width)))\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 28\n",
    "velocity = make_velocity_function(\"a sitting orange cat in winter\", height, width)\n",
    "latent = make_empty_latent(height, width)\n",
    "t_shift = calculate_timestep_shift(torch.linspace(1.0, 0.0, n_timesteps + 1), latent.shape[1])\n",
    "latent = sample_preview(latent, velocity, t_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion\n",
    "\n",
    "Almost useless with less than 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_inv = 0.8\n",
    "latent = image_to_latent(img)\n",
    "add_gallery(img, \"Original\")\n",
    "t_shift = calculate_timestep_shift(\n",
    "    torch.linspace(0.0, t_inv, n_timesteps + 1), latent.shape[1]\n",
    ")\n",
    "latent_inv = sample_preview(latent, velocity, t_shift)\n",
    "add_gallery(latent_to_image(latent_inv, height, width), f\"Inverse t={t_inv}\")\n",
    "t_shift = calculate_timestep_shift(\n",
    "    torch.linspace(t_inv, 0.0, n_timesteps + 1), latent.shape[1]\n",
    ")\n",
    "latent = sample_preview(latent_inv, velocity, t_shift)\n",
    "add_gallery(latent_to_image(latent, height, width), f\"Reconstructed\")\n",
    "show_gallery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interp Editor\n",
    "\n",
    "Original FlowGrad:\n",
    "\n",
    "$$\n",
    "Z_{i+1} = Z_i + u_i + (t_{i+1}-t_i) \\cdot v(Z_i, t_i)\n",
    "$$\n",
    "\n",
    "Interp Editor:\n",
    "\n",
    "$$\n",
    "Z_{i+1} = Z_i + (t_{i+1}-t_i) \\cdot v'(Z_i, t_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "v'(Z, t) = \\epsilon v(Z, t) + (1-\\epsilon) v_{\\text{target}}(Z, t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_{\\text{target}}(Z_i, t_i) = \\frac{1}{t}(X_{\\text{target}} - Z_i)\n",
    "$$\n",
    "\n",
    "To apply FlowGrad to the Interp Editor, we have 2 options to put $u_i$:\n",
    "\n",
    "1. (Same as Original) As addition to $Z_i$. $\\epsilon$ is only used to initialize $u_i$.\n",
    "2. Replace $v_{\\text{target}}$ with $u_i$. $\\epsilon$ is always used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eta_schedule(start_t, end_t, eta, eta_trend, alpha=0.0):\n",
    "    def eta_schedule(t):\n",
    "        if t < start_t or t > end_t:\n",
    "            return 0\n",
    "        tau = (t - start_t) / (end_t - start_t)\n",
    "        if eta_trend == \"constant\":\n",
    "            return eta\n",
    "        elif eta_trend == \"linear_decrease\":\n",
    "            return eta * (1 - tau)\n",
    "        elif eta_trend == \"exponential_decrease\":\n",
    "            if abs(alpha) < 1e-5:\n",
    "                return eta * (1 - tau)\n",
    "            else:\n",
    "                numerator = math.exp(alpha * tau) - 1\n",
    "                denominator = math.exp(alpha) - 1\n",
    "                return eta * (1 - (numerator / denominator))\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported eta_trend: {eta_trend}\")\n",
    "\n",
    "    if start_t > end_t:\n",
    "        start_t = 1 - start_t\n",
    "        end_t = 1 - end_t\n",
    "        return lambda t: eta_schedule(1 - t)\n",
    "    return eta_schedule\n",
    "\n",
    "\n",
    "eta_fn = make_eta_schedule(1, 0.75, 0.93, \"exponential_decrease\", alpha=2.0)\n",
    "eta = [eta_fn(t) for t in torch.linspace(1, 0, 100)]\n",
    "plt.plot(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flowgrad1_init_u(latent, latent_target, velocity, t, eta_fn):\n",
    "    u = []\n",
    "    n_timesteps = len(t) - 1\n",
    "    latent_display = display(show_image_tensor(latent_to_image(latent, height, width, vae=taef1)), display_id=\"latent_preview\")\n",
    "    for i in tqdm(range(n_timesteps)):\n",
    "        v_orig = velocity(latent, t[i]).to(torch.float32)\n",
    "        latent = latent.to(torch.float32)\n",
    "        \n",
    "        eta = eta_fn(t[i])\n",
    "        v_target = (latent - latent_target) / t[i]\n",
    "        v_total = eta * v_target + (1 - eta) * v_orig\n",
    "        u_i = v_total - v_orig\n",
    "        u.append(u_i)\n",
    "        latent = latent + v_total * (t[i+1] - t[i])\n",
    "        \n",
    "        latent = latent.to(dtype)\n",
    "        latent_display.update(show_image_tensor(latent_to_image(latent, height, width, vae=taef1)))\n",
    "        \n",
    "    latent_display.update(show_image_tensor(latent_to_image(latent, height, width)))\n",
    "    return u, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = make_empty_latent(height, width)\n",
    "latent_target = image_to_latent(img).to(torch.float32)\n",
    "velocity = make_velocity_function(\"a photo of a sitting tiger\", height, width)\n",
    "t = calculate_timestep_shift(torch.linspace(1.0, 0.0, n_timesteps + 1), latent.shape[1])\n",
    "eta_fn = make_eta_schedule(1, 0.75, 0.93, \"exponential_decrease\", alpha=2.0)\n",
    "u, latent_out = flowgrad1_init_u(latent, latent_target, velocity, t, eta_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
