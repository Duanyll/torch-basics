{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grounded_sam_helpers import grounded_segmentation, plot_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "labels = [\"car\", \"shadow of car\"]\n",
    "threshold = 0.3\n",
    "\n",
    "detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "segmenter_id = \"facebook/sam-vit-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array, detections = grounded_segmentation(\n",
    "    image=image_url,\n",
    "    labels=labels,\n",
    "    threshold=threshold,\n",
    "    polygon_refinement=True,\n",
    "    detector_id=detector_id,\n",
    "    segmenter_id=segmenter_id,\n",
    ")\n",
    "plot_detections(image_array, detections, \"cute_cats.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"mask-generation\", model=\"facebook/sam-vit-huge\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sam_helpers import show_masks_on_image\n",
    "outputs = generator(raw_image, points_per_batch=64)\n",
    "masks = outputs[\"masks\"]\n",
    "show_masks_on_image(raw_image, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw_image to bchw tensor\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "image = to_tensor(raw_image).unsqueeze(0)\n",
    "b, c, h, w = image.shape\n",
    "# device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "grid = torch.stack(\n",
    "    torch.meshgrid(\n",
    "        torch.arange(h, device=device, dtype=torch.float32),\n",
    "        torch.arange(w, device=device, dtype=torch.float32),\n",
    "        indexing=\"ij\",\n",
    "    ),\n",
    "    dim=-1,\n",
    ")\n",
    "grid = grid.unsqueeze(0).repeat(b, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(outputs[\"masks\"][1], dtype=torch.float32)\n",
    "plt.imshow(mask, cmap=\"gray\", interpolation=\"lanczos\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, reduce\n",
    "mask = torch.tensor(outputs[\"masks\"][1], dtype=torch.float32, device=device)\n",
    "mass_center = reduce(rearrange(mask, \"h w -> 1 h w 1\") * grid, \"b h w c -> b c\", \"sum\") / reduce(mask, \"h w -> 1\", \"sum\")\n",
    "ci, cj = mass_center[0].tolist()\n",
    "ci, cj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transform_matrices import translation, rotation, scale, shear\n",
    "grid_homo = torch.cat((grid, torch.ones(b, h, w, 1, device=device)), dim=3)\n",
    "# Move mass center to 0, 0\n",
    "transforms = (\n",
    "    translation(ci, cj) \n",
    "    @ rotation(0.03) \n",
    "    @ scale(1.01, 1) \n",
    "    @ shear(0.00, 0.02) \n",
    "    @ translation(-ci + 20, -cj + 20)\n",
    ")\n",
    "grid_homo = grid_homo @ transforms.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out = grid_homo[..., :2] / grid_homo[..., 2:]\n",
    "grid_out = grid_out / torch.tensor([h - 1, w - 1], device=device, dtype=torch.float32)\n",
    "grid_out = grid_out * 2 - 1\n",
    "# Flip the last dimension to match the grid_sample format\n",
    "grid_out = grid_out.flip(-1)\n",
    "out = torch.nn.functional.grid_sample(image, grid_out, align_corners=True)\n",
    "out_np = rearrange(out.cpu().numpy(), \"1 c h w -> h w c\")\n",
    "plt.imshow(out_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_out = torch.nn.functional.grid_sample(\n",
    "    rearrange(mask, \"h w -> 1 1 h w\"), grid_out, align_corners=True\n",
    ").clamp(0, 1)\n",
    "out_composed = image * (1 - mask_out) + out * mask_out\n",
    "out_composed_np = rearrange(out_composed.cpu().numpy(), \"1 c h w -> h w c\")\n",
    "plt.imshow(out_composed_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask_structure(masks):\n",
    "    n = len(masks)\n",
    "    areas = reduce(masks, \"n h w -> n\", \"sum\")\n",
    "    order = areas.argsort(descending=True)\n",
    "    masks = masks[order]\n",
    "    # Determine the parent-child relationship\n",
    "    parent = torch.full((len(masks),), -1, dtype=torch.long)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        th = masks[i].sum() * 0.9\n",
    "        for j in range(i - 1, -1, -1):\n",
    "            if (masks[i] & masks[j]).sum() >= th:\n",
    "                parent[i] = j\n",
    "                break\n",
    "    return masks, parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "masks = torch.tensor(np.array(outputs[\"masks\"]), device=device)\n",
    "masks, parent = generate_mask_structure(masks)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10, 6, figsize=(12, 20))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i >= len(masks):\n",
    "        break\n",
    "    ax.imshow(masks[i], cmap=\"gray\", interpolation=\"lanczos\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.title.set_text(f\"{i} ({parent[i].item()})\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save masks[2] as a PNG file\n",
    "mask_pil = Image.fromarray((masks[2].cpu().numpy() * 255).astype(np.uint8))\n",
    "mask_pil.save(\"mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
